{
  "questions": [
    {
      "question": "What are Large Language Models (LLMs) primarily trained on?",
      "options": {
        "a": "Only conversational data",
        "b": "Mathematical equations",
        "c": "Small, curated datasets",
        "d": "Vast datasets to learn statistical patterns"
      },
      "answer": "d"
    },
    {
      "question": "How many parameters do modern LLMs typically have?",
      "options": {
        "a": "Billions",
        "b": "Hundreds",
        "c": "Millions",
        "d": "Thousands"
      },
      "answer": "a"
    },
    {
      "question": "What are emergent properties in LLMs?",
      "options": {
        "a": "Bugs in the model",
        "b": "Simple language processing capabilities",
        "c": "Capabilities beyond simple language processing that arise from scale",
        "d": "Pre-programmed features"
      },
      "answer": "c"
    },
    {
      "question": "How do users primarily interact with LLMs?",
      "options": {
        "a": "Using binary code",
        "b": "Through mathematical formulas",
        "c": "Using natural language prompts",
        "d": "Through programming languages only"
      },
      "answer": "c"
    },
    {
      "question": "What is the typical context window size for LLM prompts?",
      "options": {
        "a": "A few hundred words",
        "b": "A few thousand words",
        "c": "Unlimited",
        "d": "Only one sentence"
      },
      "answer": "b"
    },
    {
      "question": "Which of the following is NOT a typical application of LLMs?",
      "options": {
        "a": "Summarizing dialogues",
        "b": "Writing essays",
        "c": "Hardware repair",
        "d": "Translation tasks"
      },
      "answer": "c"
    },
    {
      "question": "What can LLMs translate natural language into?",
      "options": {
        "a": "Only other natural languages",
        "b": "Mathematical equations only",
        "c": "Machine code and other languages",
        "d": "Only programming languages"
      },
      "answer": "c"
    },
    {
      "question": "What is named entity recognition?",
      "options": {
        "a": "Deleting entities from text",
        "b": "Identifying people and places in text",
        "c": "Translating names",
        "d": "Creating new names"
      },
      "answer": "b"
    },
    {
      "question": "How can LLMs be enhanced beyond their training data?",
      "options": {
        "a": "By connecting to external data sources or APIs",
        "b": "By limiting their vocabulary",
        "c": "By making them larger only",
        "d": "By reducing their parameters"
      },
      "answer": "a"
    },
    {
      "question": "What happens to LLM performance as the number of parameters increases?",
      "options": {
        "a": "It decreases",
        "b": "It stays the same",
        "c": "It becomes unpredictable",
        "d": "It improves"
      },
      "answer": "d"
    },
    {
      "question": "What was a major limitation of RNNs for generative tasks?",
      "options": {
        "a": "Perfect performance",
        "b": "Too much memory usage",
        "c": "Limited compute and memory, struggling with context",
        "d": "Too fast processing"
      },
      "answer": "c"
    },
    {
      "question": "Why did RNNs struggle with next-word prediction?",
      "options": {
        "a": "Insufficient context from only a few preceding words",
        "b": "They had too much context",
        "c": "They processed too quickly",
        "d": "They were too accurate"
      },
      "answer": "a"
    },
    {
      "question": "What makes language processing complex for models?",
      "options": {
        "a": "Simple grammar rules",
        "b": "Limited vocabulary",
        "c": "Short sentences",
        "d": "Homonyms and syntactic ambiguity"
      },
      "answer": "d"
    },
    {
      "question": "What paper introduced the transformer architecture?",
      "options": {
        "a": "\"Neural Networks Explained\"",
        "b": "\"Attention is All You Need\"",
        "c": "\"Language Model Basics\"",
        "d": "\"Deep Learning Revolution\""
      },
      "answer": "b"
    },
    {
      "question": "In what year was the transformer architecture introduced?",
      "options": {
        "a": "2018",
        "b": "2015",
        "c": "2017",
        "d": "2016"
      },
      "answer": "c"
    },
    {
      "question": "What key advantages do transformers have over RNNs?",
      "options": {
        "a": "Slower processing",
        "b": "Less memory usage",
        "c": "Efficient scaling, parallel processing, focus on word meaning",
        "d": "Simpler architecture"
      },
      "answer": "c"
    },
    {
      "question": "What is self-attention in transformers?",
      "options": {
        "a": "The model focusing on itself",
        "b": "A type of error correction",
        "c": "A debugging mechanism",
        "d": "Learning the relevance of each word to every other word"
      },
      "answer": "d"
    },
    {
      "question": "When are attention weights learned in transformers?",
      "options": {
        "a": "During training",
        "b": "Never learned",
        "c": "After deployment",
        "d": "During inference"
      },
      "answer": "a"
    },
    {
      "question": "What must happen to words before processing in transformers?",
      "options": {
        "a": "They must be shortened",
        "b": "They must be capitalized",
        "c": "They must be tokenized into numerical representations",
        "d": "They must be translated"
      },
      "answer": "c"
    },
    {
      "question": "What does the embedding layer do?",
      "options": {
        "a": "Deletes unnecessary words",
        "b": "Counts words",
        "c": "Translates languages",
        "d": "Converts token IDs into high-dimensional vectors"
      },
      "answer": "d"
    },
    {
      "question": "What is multi-headed self-attention?",
      "options": {
        "a": "A memory optimization technique",
        "b": "A single attention mechanism",
        "c": "A type of error checking",
        "d": "Multiple sets of attention weights to learn different language aspects"
      },
      "answer": "d"
    },
    {
      "question": "In the transformer prediction process, what does the encoder do?",
      "options": {
        "a": "Creates a deep representation of input",
        "b": "Generates final output",
        "c": "Translates languages directly",
        "d": "Deletes unnecessary information"
      },
      "answer": "a"
    },
    {
      "question": "What does the decoder do in transformers?",
      "options": {
        "a": "Counts tokens",
        "b": "Deletes tokens",
        "c": "Generates new tokens based on encoder's understanding",
        "d": "Encodes input sequences"
      },
      "answer": "c"
    },
    {
      "question": "Which model type is BERT an example of?",
      "options": {
        "a": "Decoder-only",
        "b": "Encoder-only",
        "c": "Encoder-decoder",
        "d": "Hybrid model"
      },
      "answer": "b"
    },
    {
      "question": "What type of tasks are encoder-only models like BERT good for?",
      "options": {
        "a": "Summarization",
        "b": "Text generation",
        "c": "Translation",
        "d": "Sentiment analysis"
      },
      "answer": "d"
    },
    {
      "question": "Which models are examples of encoder-decoder architecture?",
      "options": {
        "a": "BERT and RoBERTa",
        "b": "Only T5",
        "c": "GPT and BLOOM",
        "d": "BART and T5"
      },
      "answer": "d"
    },
    {
      "question": "What type of model is GPT?",
      "options": {
        "a": "Encoder-decoder",
        "b": "Encoder-only",
        "c": "Hybrid",
        "d": "Decoder-only"
      },
      "answer": "d"
    },
    {
      "question": "What replaces RNNs and CNNs in transformer architecture?",
      "options": {
        "a": "Attention-based mechanisms",
        "b": "Memory banks",
        "c": "Simpler algorithms",
        "d": "Processing units"
      },
      "answer": "a"
    },
    {
      "question": "What does self-attention help transformers capture?",
      "options": {
        "a": "No dependencies",
        "b": "Long-term dependencies",
        "c": "Short-term dependencies only",
        "d": "Only immediate relationships"
      },
      "answer": "b"
    },
    {
      "question": "What training enhancements are used in transformers?",
      "options": {
        "a": "Residual connections and layer normalization",
        "b": "No enhancements needed",
        "c": "Only residual connections",
        "d": "Only layer normalization"
      },
      "answer": "a"
    },
    {
      "question": "What is positional encoding used for?",
      "options": {
        "a": "Memory management",
        "b": "Maintaining token order without recurrent operations",
        "c": "Error correction",
        "d": "Speed optimization"
      },
      "answer": "b"
    },
    {
      "question": "What is the text input to a model called?",
      "options": {
        "a": "Completion",
        "b": "Query",
        "c": "Request",
        "d": "Prompt"
      },
      "answer": "d"
    },
    {
      "question": "What is the generated text from a model called?",
      "options": {
        "a": "Input",
        "b": "Query",
        "c": "Prompt",
        "d": "Completion"
      },
      "answer": "d"
    },
    {
      "question": "What does the context window refer to?",
      "options": {
        "a": "Processing speed",
        "b": "The model's memory",
        "c": "Total amount of text available for the prompt",
        "d": "The output length"
      },
      "answer": "c"
    },
    {
      "question": "What is prompt engineering?",
      "options": {
        "a": "Building hardware for prompts",
        "b": "Translating prompts",
        "c": "Deleting prompts",
        "d": "Developing effective prompts through multiple revisions"
      },
      "answer": "d"
    },
    {
      "question": "What is in-context learning?",
      "options": {
        "a": "Learning during training only",
        "b": "Including examples in prompts to improve performance",
        "c": "Learning without examples",
        "d": "Post-processing technique"
      },
      "answer": "b"
    },
    {
      "question": "What is zero-shot inference?",
      "options": {
        "a": "Providing a prompt without examples",
        "b": "Inference with two examples",
        "c": "Inference with one example",
        "d": "Inference with many examples"
      },
      "answer": "a"
    },
    {
      "question": "What is one-shot inference?",
      "options": {
        "a": "Inference without examples",
        "b": "Inference with multiple examples",
        "c": "Inference with two examples",
        "d": "Inference with a single example in the prompt"
      },
      "answer": "d"
    },
    {
      "question": "What is few-shot inference?",
      "options": {
        "a": "Inference with multiple examples",
        "b": "Inference with unlimited examples",
        "c": "Inference without examples",
        "d": "Inference with one example"
      },
      "answer": "a"
    },
    {
      "question": "Which models perform better at zero-shot tasks?",
      "options": {
        "a": "Medium-sized models",
        "b": "Smaller models",
        "c": "Larger models",
        "d": "All models perform equally"
      },
      "answer": "c"
    },
    {
      "question": "What might be necessary if a model struggles with performance?",
      "options": {
        "a": "Increasing speed",
        "b": "Deleting parameters",
        "c": "Reducing model size",
        "d": "Fine-tuning"
      },
      "answer": "d"
    },
    {
      "question": "What does the \"max new tokens\" parameter control?",
      "options": {
        "a": "Model size",
        "b": "Input length",
        "c": "Number of tokens generated",
        "d": "Processing speed"
      },
      "answer": "c"
    },
    {
      "question": "How does greedy decoding select words?",
      "options": {
        "a": "Selects the lowest probability word",
        "b": "Selects the highest probability word",
        "c": "Uses multiple criteria",
        "d": "Randomly"
      },
      "answer": "b"
    },
    {
      "question": "What is a limitation of greedy decoding?",
      "options": {
        "a": "Too slow",
        "b": "May lead to repetition",
        "c": "Too much variety",
        "d": "Too complex"
      },
      "answer": "b"
    },
    {
      "question": "What does random sampling do?",
      "options": {
        "a": "Ignores probabilities",
        "b": "Selects words alphabetically",
        "c": "Selects words based on probability distribution",
        "d": "Always selects the most likely word"
      },
      "answer": "c"
    },
    {
      "question": "What is top-k sampling?",
      "options": {
        "a": "Sampling from all possible tokens",
        "b": "Random selection without restrictions",
        "c": "Restricting model to k most probable tokens",
        "d": "Using only one token"
      },
      "answer": "c"
    },
    {
      "question": "What is top-p sampling based on?",
      "options": {
        "a": "Token length",
        "b": "Cumulative probabilities",
        "c": "Fixed number of tokens",
        "d": "Alphabetical order"
      },
      "answer": "b"
    },
    {
      "question": "What does the temperature parameter control?",
      "options": {
        "a": "Model processing speed",
        "b": "Input length",
        "c": "Probability distribution shape and randomness",
        "d": "Model size"
      },
      "answer": "c"
    },
    {
      "question": "What happens with lower temperature values?",
      "options": {
        "a": "Less randomness",
        "b": "Faster processing",
        "c": "More randomness",
        "d": "No change"
      },
      "answer": "a"
    },
    {
      "question": "What does a temperature of one represent?",
      "options": {
        "a": "Default distribution",
        "b": "Maximum randomness",
        "c": "No output",
        "d": "Minimum randomness"
      },
      "answer": "a"
    },
    {
      "question": "What is crucial in the project lifecycle's initial stage?",
      "options": {
        "a": "Writing documentation",
        "b": "Choosing hardware",
        "c": "Defining project scope accurately",
        "d": "Selecting colors"
      },
      "answer": "c"
    },
    {
      "question": "Why is understanding specific LLM tasks important?",
      "options": {
        "a": "Can save time and compute costs",
        "b": "For legal compliance",
        "c": "For documentation",
        "d": "For user interface design"
      },
      "answer": "a"
    },
    {
      "question": "What is a common starting point for most LLM projects?",
      "options": {
        "a": "Building custom hardware",
        "b": "Collecting new data",
        "c": "Using existing pre-trained models",
        "d": "Training from scratch"
      },
      "answer": "c"
    },
    {
      "question": "What are model hubs like Hugging Face useful for?",
      "options": {
        "a": "Training models",
        "b": "Data collection",
        "c": "Providing model cards with essential details",
        "d": "Hardware selection"
      },
      "answer": "c"
    },
    {
      "question": "What is the initial training phase called?",
      "options": {
        "a": "Fine-tuning",
        "b": "Post-training",
        "c": "Pre-training",
        "d": "Meta-training"
      },
      "answer": "c"
    },
    {
      "question": "What does pre-training involve?",
      "options": {
        "a": "Manual programming",
        "b": "Learning from vast amounts of unstructured textual data",
        "c": "Only supervised learning",
        "d": "Small, structured datasets"
      },
      "answer": "b"
    },
    {
      "question": "What do encoder-only models like BERT use for pre-training?",
      "options": {
        "a": "Causal language modeling",
        "b": "Masked language modeling",
        "c": "Summarization tasks",
        "d": "Translation tasks"
      },
      "answer": "b"
    },
    {
      "question": "What are encoder-only models suitable for?",
      "options": {
        "a": "Translation",
        "b": "Creative writing",
        "c": "Text generation",
        "d": "Sentiment analysis"
      },
      "answer": "d"
    },
    {
      "question": "What do decoder-only models like GPT use for pre-training?",
      "options": {
        "a": "Causal language modeling",
        "b": "Translation tasks",
        "c": "Classification tasks",
        "d": "Masked language modeling"
      },
      "answer": "a"
    },
    {
      "question": "What are sequence-to-sequence models effective for?",
      "options": {
        "a": "Only generation",
        "b": "Translation and summarization",
        "c": "Only sentiment analysis",
        "d": "Only classification"
      },
      "answer": "b"
    },
    {
      "question": "What is generally true about larger models?",
      "options": {
        "a": "They are easier to train",
        "b": "They perform better but are costly to train",
        "c": "They perform worse",
        "d": "They require less memory"
      },
      "answer": "b"
    },
    {
      "question": "What memory challenge occurs when training LLMs?",
      "options": {
        "a": "Only CPU memory matters",
        "b": "Memory is not important",
        "c": "Out-of-memory errors due to high GPU memory requirements",
        "d": "Too little memory usage"
      },
      "answer": "c"
    },
    {
      "question": "How much GPU RAM does a 1 billion parameter model at 32-bit precision require?",
      "options": {
        "a": "6 GB",
        "b": "Approximately 24 GB",
        "c": "12 GB",
        "d": "48 GB"
      },
      "answer": "b"
    },
    {
      "question": "What is quantization?",
      "options": {
        "a": "Increasing model precision",
        "b": "Reducing precision of model weights to lower bit representations",
        "c": "Adding more parameters",
        "d": "Increasing model size"
      },
      "answer": "b"
    },
    {
      "question": "How much memory reduction does FP16 provide compared to FP32?",
      "options": {
        "a": "No reduction",
        "b": "Three-fourths reduction",
        "c": "Reduces by half",
        "d": "One-fourth"
      },
      "answer": "c"
    },
    {
      "question": "How much memory reduction does INT8 provide compared to FP32?",
      "options": {
        "a": "Half",
        "b": "No reduction",
        "c": "One-eighth",
        "d": "One-fourth"
      },
      "answer": "d"
    },
    {
      "question": "What is BFLOAT16?",
      "options": {
        "a": "8-bit floating point",
        "b": "Standard 16-bit integer",
        "c": "Hybrid format maintaining FP32's dynamic range with reduced memory",
        "d": "32-bit integer"
      },
      "answer": "c"
    },
    {
      "question": "What measures compute resources needed for training?",
      "options": {
        "a": "GB per second",
        "b": "CPU cycles",
        "c": "Memory bandwidth",
        "d": "PetaFLOP per second day"
      },
      "answer": "d"
    },
    {
      "question": "What does one petaFLOP per second day represent?",
      "options": {
        "a": "One quadrillion floating point operations per second for one day",
        "b": "One trillion operations per hour",
        "c": "One billion operations per second for one day",
        "d": "One million operations per day"
      },
      "answer": "a"
    },
    {
      "question": "What relationship exists between compute budget, model size, and dataset size?",
      "options": {
        "a": "Power-law relationship",
        "b": "Inverse relationship",
        "c": "Linear relationship",
        "d": "No relationship"
      },
      "answer": "a"
    },
    {
      "question": "According to the Chinchilla paper, what is optimal for model training?",
      "options": {
        "a": "Smallest possible datasets",
        "b": "Largest possible models",
        "c": "Equal dataset size and parameter count",
        "d": "Dataset sizes about 20 times the number of model parameters"
      },
      "answer": "d"
    },
    {
      "question": "What trend is emerging in model design?",
      "options": {
        "a": "Optimizing model design rather than simply increasing size",
        "b": "Reducing all model parameters",
        "c": "Only increasing model size",
        "d": "Ignoring dataset size"
      },
      "answer": "a"
    },
    {
      "question": "When is domain-specific pretraining essential?",
      "options": {
        "a": "Never necessary",
        "b": "Only for translation",
        "c": "When working with specialized language like legal or medical terminology",
        "d": "For all applications"
      },
      "answer": "c"
    },
    {
      "question": "What is an example of specialized legal terminology?",
      "options": {
        "a": "\"Data science\"",
        "b": "\"Hello world\"",
        "c": "\"Machine learning\"",
        "d": "\"Mens rea\" and \"res judicata\""
      },
      "answer": "d"
    },
    {
      "question": "What is BloombergGPT designed for?",
      "options": {
        "a": "Finance domain",
        "b": "Medical applications",
        "c": "Legal research",
        "d": "General conversation"
      },
      "answer": "a"
    },
    {
      "question": "What is instruction fine-tuning?",
      "options": {
        "a": "Adjusting pre-trained models to better respond to user prompts",
        "b": "Deleting model parameters",
        "c": "Increasing model size",
        "d": "Teaching models hardware instructions"
      },
      "answer": "a"
    },
    {
      "question": "What are the two main types of fine-tuning discussed?",
      "options": {
        "a": "Manual and automatic fine-tuning",
        "b": "Fast and slow fine-tuning",
        "c": "Instruction fine-tuning and application-specific fine-tuning",
        "d": "Large and small fine-tuning"
      },
      "answer": "c"
    },
    {
      "question": "What does PEFT stand for?",
      "options": {
        "a": "Practical Enhanced Fine Tuning",
        "b": "Pre-trained Efficient Fine Tuning",
        "c": "Post-Evaluation Fine Tuning",
        "d": "Parameter Efficient Fine-Tuning"
      },
      "answer": "d"
    },
    {
      "question": "What is LoRA?",
      "options": {
        "a": "Low-Rank Adaptation",
        "b": "Language-Oriented Research",
        "c": "Linear Regression Algorithm",
        "d": "Large-scale Optimization"
      },
      "answer": "a"
    },
    {
      "question": "What type of learning process is fine-tuning?",
      "options": {
        "a": "Semi-supervised learning",
        "b": "Reinforcement learning",
        "c": "Supervised learning",
        "d": "Unsupervised learning"
      },
      "answer": "c"
    },
    {
      "question": "How does instruction fine-tuning prepare datasets?",
      "options": {
        "a": "Prompt-completion pairs with relevant instructions",
        "b": "Random text pairs",
        "c": "Images and text",
        "d": "Single words only"
      },
      "answer": "a"
    },
    {
      "question": "How many examples are typically needed for single-task fine-tuning?",
      "options": {
        "a": "500-1,000",
        "b": "100,000+",
        "c": "50-100",
        "d": "10,000-50,000"
      },
      "answer": "a"
    },
    {
      "question": "What is catastrophic forgetting?",
      "options": {
        "a": "Complete memory loss",
        "b": "Models losing ability to perform other tasks after fine-tuning",
        "c": "Models forgetting how to start",
        "d": "Forgetting training data"
      },
      "answer": "b"
    },
    {
      "question": "What causes catastrophic forgetting?",
      "options": {
        "a": "Modification of original LLM weights during fine-tuning",
        "b": "Hardware issues",
        "c": "Too little training",
        "d": "Too much data"
      },
      "answer": "a"
    },
    {
      "question": "What is multitask fine-tuning?",
      "options": {
        "a": "Using datasets with multiple tasks to maintain general capabilities",
        "b": "Using multiple GPUs",
        "c": "Training multiple models",
        "d": "Training on single tasks only"
      },
      "answer": "a"
    },
    {
      "question": "How many examples does multitask fine-tuning typically require?",
      "options": {
        "a": "10-50",
        "b": "50,000-100,000",
        "c": "500-1,000",
        "d": "1,000,000+"
      },
      "answer": "b"
    },
    {
      "question": "What are FLAN models?",
      "options": {
        "a": "Data preprocessing tools",
        "b": "Models fine-tuned using multitask instruction",
        "c": "Models trained on single tasks",
        "d": "Hardware accelerators"
      },
      "answer": "b"
    },
    {
      "question": "How many datasets was FLAN-T5 fine-tuned on?",
      "options": {
        "a": "1,000 datasets",
        "b": "473 datasets",
        "c": "50 datasets",
        "d": "100 datasets"
      },
      "answer": "b"
    },
    {
      "question": "What is the SAMSum dataset used for?",
      "options": {
        "a": "Training models to summarize dialogues",
        "b": "Sentiment analysis",
        "c": "Image recognition",
        "d": "Translation tasks"
      },
      "answer": "a"
    },
    {
      "question": "How many conversations does SAMSum contain?",
      "options": {
        "a": "16,000",
        "b": "32,000",
        "c": "5,000",
        "d": "8,000"
      },
      "answer": "a"
    },
    {
      "question": "What does ROUGE primarily assess?",
      "options": {
        "a": "Sentiment accuracy",
        "b": "Grammar correctness",
        "c": "Translation quality",
        "d": "Quality of automatically generated summaries"
      },
      "answer": "d"
    },
    {
      "question": "What does BLEU evaluate?",
      "options": {
        "a": "Sentiment analysis",
        "b": "Named entity recognition",
        "c": "Summary quality",
        "d": "Machine-translated text quality"
      },
      "answer": "d"
    },
    {
      "question": "What is a unigram?",
      "options": {
        "a": "A sentence",
        "b": "Two words",
        "c": "Three words",
        "d": "A single word"
      },
      "answer": "d"
    },
    {
      "question": "What is a bigram?",
      "options": {
        "a": "Two words",
        "b": "A single word",
        "c": "Four words",
        "d": "Three words"
      },
      "answer": "a"
    },
    {
      "question": "What does GLUE stand for?",
      "options": {
        "a": "Global Learning Understanding Evaluation",
        "b": "General Language Understanding Evaluation",
        "c": "Grammar and Language Understanding Evaluation",
        "d": "Generative Language User Evaluation"
      },
      "answer": "b"
    },
    {
      "question": "What is SuperGLUE?",
      "options": {
        "a": "A hardware acceleration tool",
        "b": "A successor to GLUE with more challenging tasks",
        "c": "An enhanced version of GLUE",
        "d": "A completely different benchmark"
      },
      "answer": "b"
    },
    {
      "question": "What does MMLU test?",
      "options": {
        "a": "Only language translation",
        "b": "Single domain knowledge",
        "c": "Only mathematical skills",
        "d": "Wide range of knowledge and problem-solving abilities"
      },
      "answer": "d"
    },
    {
      "question": "How many tasks does BIG-bench consist of?",
      "options": {
        "a": "500 tasks",
        "b": "204 tasks",
        "c": "100 tasks",
        "d": "50 tasks"
      },
      "answer": "b"
    },
    {
      "question": "What does HELM evaluate?",
      "options": {
        "a": "Only memory usage",
        "b": "Only accuracy",
        "c": "Only speed",
        "d": "Models on various metrics including fairness and bias"
      },
      "answer": "d"
    },
    {
      "question": "What is a key advantage of PEFT over full fine-tuning?",
      "options": {
        "a": "Requires more data",
        "b": "Reduces memory requirements and computational costs",
        "c": "Worse performance",
        "d": "More complex implementation"
      },
      "answer": "b"
    },
    {
      "question": "What are the three types of PEFT methods mentioned?",
      "options": {
        "a": "Fast, medium, slow",
        "b": "Simple, complex, hybrid",
        "c": "Local, global, mixed",
        "d": "Selective, reparameterization, additive"
      },
      "answer": "d"
    },
    {
      "question": "What do selective PEFT methods do?",
      "options": {
        "a": "Add new parameters",
        "b": "Fine-tune a subset of original LLM parameters",
        "c": "Reorganize parameters",
        "d": "Delete parameters"
      },
      "answer": "b"
    },
    {
      "question": "What do reparameterization methods create?",
      "options": {
        "a": "Low-rank transformations of original weights",
        "b": "Simplified architectures",
        "c": "Copies of original models",
        "d": "New models from scratch"
      },
      "answer": "a"
    },
    {
      "question": "What do additive methods do?",
      "options": {
        "a": "Modify existing components",
        "b": "Simplify the model",
        "c": "Remove components",
        "d": "Introduce new trainable components while keeping original weights frozen"
      },
      "answer": "d"
    },
    {
      "question": "How does LoRA work?",
      "options": {
        "a": "Freezes original parameters and introduces smaller rank decomposition matrices",
        "b": "Duplicates the model",
        "c": "Trains all parameters",
        "d": "Deletes unnecessary parameters"
      },
      "answer": "a"
    },
    {
      "question": "Where is LoRA primarily applied for best results?",
      "options": {
        "a": "All layers equally",
        "b": "Input layers only",
        "c": "Self-attention layers",
        "d": "Output layers only"
      },
      "answer": "c"
    },
    {
      "question": "What rank range provides good balance in LoRA?",
      "options": {
        "a": "50-100",
        "b": "1-2",
        "c": "4-32",
        "d": "100-200"
      },
      "answer": "c"
    },
    {
      "question": "What is prompt tuning?",
      "options": {
        "a": "Adding trainable tokens (soft prompts) to input",
        "b": "Manual prompt adjustment",
        "c": "Translating prompts",
        "d": "Deleting prompts"
      },
      "answer": "a"
    },
    {
      "question": "How does prompt tuning differ from prompt engineering?",
      "options": {
        "a": "Prompt tuning is simpler",
        "b": "There's no difference",
        "c": "It's the same process",
        "d": "Prompt tuning automates optimization while engineering is manual"
      },
      "answer": "d"
    },
    {
      "question": "With what model sizes does prompt tuning perform well?",
      "options": {
        "a": "All sizes equally",
        "b": "Medium models only",
        "c": "Large models (around 10B+ parameters)",
        "d": "Small models (under 1B parameters)"
      },
      "answer": "c"
    },
    {
      "question": "What does HHH stand for in AI principles?",
      "options": {
        "a": "Happy, Helpful, Honest",
        "b": "Hard, Heavy, High",
        "c": "Human, Hybrid, Holistic",
        "d": "Helpfulness, Honesty, Harmlessness"
      },
      "answer": "d"
    },
    {
      "question": "What is RLHF?",
      "options": {
        "a": "Rapid Learning from Human Feedback",
        "b": "Reinforcement Learning from Human Feedback",
        "c": "Real-time Learning from Human Feedback",
        "d": "Recursive Learning from Human Feedback"
      },
      "answer": "b"
    },
    {
      "question": "What is RLHF?",
      "options": {
        "a": "Rapid Learning from Human Feedback",
        "b": "Recursive Learning from Human Feedback",
        "c": "Real-time Learning from Human Feedback",
        "d": "Reinforcement Learning from Human Feedback"
      },
      "answer": "d"
    },
    {
      "question": "In reinforcement learning, what does an agent try to maximize?",
      "options": {
        "a": "Memory usage",
        "b": "Speed",
        "c": "Model size",
        "d": "Cumulative rewards"
      },
      "answer": "d"
    },
    {
      "question": "What is the first step in RLHF?",
      "options": {
        "a": "Train a reward model",
        "b": "Choose a model and generate responses from prompts",
        "c": "Collect user feedback",
        "d": "Deploy the model"
      },
      "answer": "b"
    },
    {
      "question": "How is human feedback typically collected in RLHF?",
      "options": {
        "a": "Through surveys only",
        "b": "Labelers rank completions based on specific criteria",
        "c": "Random selection",
        "d": "Automatic evaluation only"
      },
      "answer": "b"
    },
    {
      "question": "What are pairwise comparisons used for?",
      "options": {
        "a": "Comparing model sizes",
        "b": "Training the reward model",
        "c": "Speed testing",
        "d": "Memory optimization"
      },
      "answer": "b"
    },
    {
      "question": "What does the reward model do once trained?",
      "options": {
        "a": "Acts as a binary classifier to evaluate completions",
        "b": "Translates languages",
        "c": "Optimizes hardware",
        "d": "Generates text"
      },
      "answer": "a"
    },
    {
      "question": "What algorithm is commonly used in the RLHF process?",
      "options": {
        "a": "Gradient descent",
        "b": "Proximal Policy Optimization (PPO)",
        "c": "Random search",
        "d": "Genetic algorithms"
      },
      "answer": "b"
    },
    {
      "question": "What is reward hacking?",
      "options": {
        "a": "Sharing rewards",
        "b": "Reducing rewards",
        "c": "Stealing rewards",
        "d": "Model learning to maximize rewards in unintended ways"
      },
      "answer": "d"
    },
    {
      "question": "What is KL divergence used for in RLHF?",
      "options": {
        "a": "Speed measurement",
        "b": "Error counting",
        "c": "Memory calculation",
        "d": "Measuring how much the updated model diverges from reference"
      },
      "answer": "d"
    },
    {
      "question": "What is Constitutional AI?",
      "options": {
        "a": "Government regulation of AI",
        "b": "Using rules to guide model behavior and self-critique",
        "c": "Legal frameworks",
        "d": "Hardware specifications"
      },
      "answer": "b"
    },
    {
      "question": "Who proposed Constitutional AI?",
      "options": {
        "a": "Anthropic researchers",
        "b": "Microsoft researchers",
        "c": "Google researchers",
        "d": "OpenAI researchers"
      },
      "answer": "a"
    },
    {
      "question": "What does RLAIF stand for?",
      "options": {
        "a": "Reinforcement Learning from Artificial Intelligence Feedback",
        "b": "Rapid Learning from AI Feedback",
        "c": "Recursive Learning from AI Integration",
        "d": "Real-time Learning from AI Input"
      },
      "answer": "a"
    },
    {
      "question": "What is model distillation?",
      "options": {
        "a": "Deleting model components",
        "b": "Removing water from models",
        "c": "Training a smaller student model using a larger teacher model",
        "d": "Increasing model size"
      },
      "answer": "c"
    },
    {
      "question": "What is model pruning?",
      "options": {
        "a": "Eliminating redundant weights that contribute little to performance",
        "b": "Adding more parameters",
        "c": "Increasing model complexity",
        "d": "Changing model architecture"
      },
      "answer": "a"
    },
    {
      "question": "What is a major limitation of LLMs regarding knowledge?",
      "options": {
        "a": "Knowledge cutoff - outdated information",
        "b": "No limitations",
        "c": "Perfect knowledge",
        "d": "Too much knowledge"
      },
      "answer": "a"
    },
    {
      "question": "What mathematical limitation do LLMs have?",
      "options": {
        "a": "Perfect calculation ability",
        "b": "Only addition problems",
        "c": "No mathematical capability",
        "d": "Struggle with complex calculations"
      },
      "answer": "d"
    },
    {
      "question": "What does RAG stand for?",
      "options": {
        "a": "Recursive AI Growth",
        "b": "Rapid AI Generation",
        "c": "Retrieval Augmented Generation",
        "d": "Real-time AI Guidance"
      },
      "answer": "c"
    },
    {
      "question": "What does RAG help LLMs overcome?",
      "options": {
        "a": "Size limitations",
        "b": "Knowledge cutoffs and improve accuracy",
        "c": "Speed issues",
        "d": "Memory problems"
      },
      "answer": "b"
    },
    {
      "question": "What components does RAG implementation involve?",
      "options": {
        "a": "Manual lookup only",
        "b": "Only databases",
        "c": "Only vector stores",
        "d": "Query encoder and external data source"
      },
      "answer": "d"
    },
    {
      "question": "In the context of LLMs, what is the primary purpose of a reward model?",
      "options": {
        "a": "To evaluate outputs and update the LLM's weights for alignment with human preferences",
        "b": "To generate new text completions",
        "c": "To store training data",
        "d": "To compress model size"
      },
      "answer": "a"
    },
    {
      "question": "What is the first step in model selection and data preparation for RLHF?",
      "options": {
        "a": "Choose a model with capabilities relevant to your task",
        "b": "Train the reward model",
        "c": "Collect human feedback",
        "d": "Convert ranking data to pairwise comparisons"
      },
      "answer": "a"
    },
    {
      "question": "How do human labelers typically provide feedback on generated completions?",
      "options": {
        "a": "By providing numerical scores only",
        "b": "By writing detailed essays",
        "c": "By rewriting the completions",
        "d": "By ranking the completions based on specific criteria"
      },
      "answer": "d"
    },
    {
      "question": "Why are detailed instructions important for human labelers?",
      "options": {
        "a": "To speed up the process",
        "b": "To ensure they understand the task and criteria for assessment",
        "c": "To reduce costs",
        "d": "To eliminate the need for quality assurance"
      },
      "answer": "b"
    },
    {
      "question": "How is ranking data converted for reward model training?",
      "options": {
        "a": "Into numerical arrays",
        "b": "Into binary classifications only",
        "c": "Into text summaries",
        "d": "Into pairwise comparisons with assigned scores"
      },
      "answer": "d"
    },
    {
      "question": "What type of data is the reward model trained on?",
      "options": {
        "a": "Pairwise comparison data from human labelers",
        "b": "Raw text data",
        "c": "Audio recordings",
        "d": "Image data"
      },
      "answer": "a"
    },
    {
      "question": "Once trained, how does the reward model function?",
      "options": {
        "a": "As a compression tool",
        "b": "As a text generator",
        "c": "As a data storage system",
        "d": "As a binary classifier to distinguish between positive and negative classes"
      },
      "answer": "d"
    },
    {
      "question": "What function is used to convert the reward model's logits to probabilities?",
      "options": {
        "a": "Softmax",
        "b": "Sigmoid",
        "c": "Tanh",
        "d": "ReLU"
      },
      "answer": "a"
    },
    {
      "question": "What is the starting point for the reinforcement learning process?",
      "options": {
        "a": "A completely untrained model",
        "b": "Human feedback data",
        "c": "A pre-trained model that performs well on the task",
        "d": "A reward model"
      },
      "answer": "c"
    },
    {
      "question": "In the RL process, what does a higher reward value indicate?",
      "options": {
        "a": "Less aligned responses",
        "b": "Faster processing time",
        "c": "More aligned responses with human preferences",
        "d": "Lower computational cost"
      },
      "answer": "c"
    },
    {
      "question": "Which algorithm is commonly used for updating LLM weights in RLHF?",
      "options": {
        "a": "Proximal Policy Optimization (PPO)",
        "b": "Gradient descent",
        "c": "Genetic algorithms",
        "d": "Random search"
      },
      "answer": "a"
    },
    {
      "question": "What is the final model called after successful alignment with human values?",
      "options": {
        "a": "Reward model",
        "b": "Reference model",
        "c": "Human-aligned LLM",
        "d": "Base model"
      },
      "answer": "c"
    },
    {
      "question": "What is reward hacking in the context of RLHF?",
      "options": {
        "a": "Reducing reward values",
        "b": "Breaking the reward model",
        "c": "When the model learns to maximize rewards in ways that don't align with original objectives",
        "d": "Stealing reward data"
      },
      "answer": "c"
    },
    {
      "question": "What metric is used to measure how much an updated model diverges from a reference model?",
      "options": {
        "a": "Cosine similarity",
        "b": "Cross-entropy",
        "c": "Mean squared error",
        "d": "Kullback-Leibler (KL) divergence"
      },
      "answer": "d"
    },
    {
      "question": "What is the purpose of using KL divergence in the reward calculation?",
      "options": {
        "a": "To increase model speed",
        "b": "To improve text quality",
        "c": "To reduce memory usage",
        "d": "To penalize excessive divergence from the reference model"
      },
      "answer": "d"
    },
    {
      "question": "What is a major challenge of scaling human feedback?",
      "options": {
        "a": "Network bandwidth",
        "b": "Technical complexity",
        "c": "Human evaluation is resource-intensive, requiring large teams",
        "d": "Storage limitations"
      },
      "answer": "c"
    },
    {
      "question": "Who proposed the Constitutional AI approach?",
      "options": {
        "a": "Microsoft researchers",
        "b": "Google researchers",
        "c": "Researchers at Anthropic",
        "d": "OpenAI researchers"
      },
      "answer": "c"
    },
    {
      "question": "What does Constitutional AI use to guide model behavior?",
      "options": {
        "a": "A set of rules for self-critique",
        "b": "Random sampling",
        "c": "External databases",
        "d": "Human feedback only"
      },
      "answer": "a"
    },
    {
      "question": "What are the two phases of Constitutional AI training?",
      "options": {
        "a": "Supervised learning (red teaming) and reinforcement learning from AI feedback (RLAIF)",
        "b": "Data collection and model deployment",
        "c": "Pre-training and fine-tuning",
        "d": "Training and testing"
      },
      "answer": "a"
    },
    {
      "question": "What is model distillation?",
      "options": {
        "a": "Removing model weights",
        "b": "Compressing data files",
        "c": "Converting text to speech",
        "d": "Training a smaller student model using a larger teacher model"
      },
      "answer": "d"
    },
    {
      "question": "What does quantization achieve in model optimization?",
      "options": {
        "a": "Increases model size",
        "b": "Improves accuracy",
        "c": "Transforms model weights to lower precision to decrease memory footprint",
        "d": "Enhances training speed"
      },
      "answer": "c"
    },
    {
      "question": "What is the purpose of model pruning?",
      "options": {
        "a": "Increasing model complexity",
        "b": "Improving data quality",
        "c": "Eliminating redundant weights that contribute little to performance",
        "d": "Adding more parameters"
      },
      "answer": "c"
    },
    {
      "question": "What is typically the starting point for most LLM development work?",
      "options": {
        "a": "Designing new architectures",
        "b": "An existing foundation model",
        "c": "Building a model from scratch",
        "d": "Creating new training data"
      },
      "answer": "b"
    },
    {
      "question": "What is the least technical approach to improving model performance?",
      "options": {
        "a": "Prompt engineering",
        "b": "Model distillation",
        "c": "Reinforcement learning",
        "d": "Fine-tuning"
      },
      "answer": "a"
    },
    {
      "question": "What is a major limitation of LLMs regarding information currency?",
      "options": {
        "a": "They have a knowledge cutoff and outdated information",
        "b": "They require too much storage",
        "c": "They process information too slowly",
        "d": "They consume too much power"
      },
      "answer": "a"
    },
    {
      "question": "What mathematical limitation do LLMs face?",
      "options": {
        "a": "They excel at all mathematical operations",
        "b": "They only work with integers",
        "c": "They cannot handle decimal numbers",
        "d": "They struggle with complex calculations and may provide incorrect answers"
      },
      "answer": "d"
    },
    {
      "question": "What does RAG stand for?",
      "options": {
        "a": "Reinforced AI Guidance",
        "b": "Random Access Gateway",
        "c": "Retrieval Augmented Generation",
        "d": "Rapid Application Generation"
      },
      "answer": "c"
    },
    {
      "question": "What is the primary benefit of RAG?",
      "options": {
        "a": "Increases processing speed",
        "b": "Reduces model size",
        "c": "Allows LLMs to access external data sources to overcome knowledge cutoffs",
        "d": "Eliminates the need for training"
      },
      "answer": "c"
    },
    {
      "question": "What components are involved in RAG implementation?",
      "options": {
        "a": "Only a language model",
        "b": "Cloud storage only",
        "c": "A query encoder and an external data source",
        "d": "Multiple GPUs"
      },
      "answer": "c"
    },
    {
      "question": "In the ShopBot example, what role does the LLM serve?",
      "options": {
        "a": "Database management",
        "b": "User interface",
        "c": "Data storage",
        "d": "The reasoning engine generating instructions for the application"
      },
      "answer": "d"
    },
    {
      "question": "What is a common challenge LLMs face with reasoning tasks?",
      "options": {
        "a": "They are too fast",
        "b": "They are too accurate",
        "c": "They use too little memory",
        "d": "They struggle with complex reasoning tasks involving multiple steps"
      },
      "answer": "d"
    },
    {
      "question": "What is chain of thought prompting designed to do?",
      "options": {
        "a": "Speed up processing",
        "b": "Improve LLM reasoning by breaking problems into smaller, human-like steps",
        "c": "Reduce model size",
        "d": "Eliminate the need for training data"
      },
      "answer": "b"
    },
    {
      "question": "What does PAL stand for?",
      "options": {
        "a": "Predictive Analysis Logic",
        "b": "Programming Assistant Language",
        "c": "Program-Aided Language Models",
        "d": "Parallel Algorithm Learning"
      },
      "answer": "c"
    },
    {
      "question": "How does PAL address LLM mathematical limitations?",
      "options": {
        "a": "By pairing an LLM with an external code interpreter like Python",
        "b": "By using larger models",
        "c": "By pre-computing all answers",
        "d": "By avoiding mathematical problems"
      },
      "answer": "a"
    },
    {
      "question": "What is the role of an orchestrator in PAL systems?",
      "options": {
        "a": "To manage the flow of information between the LLM and interpreter",
        "b": "To play music",
        "c": "To store data",
        "d": "To provide user interfaces"
      },
      "answer": "a"
    },
    {
      "question": "What does the ReAct framework combine?",
      "options": {
        "a": "Speed and accuracy",
        "b": "Training and testing",
        "c": "Chain of thought reasoning with action planning",
        "d": "Reading and writing"
      },
      "answer": "c"
    },
    {
      "question": "Who proposed the ReAct framework?",
      "options": {
        "a": "OpenAI researchers",
        "b": "Researchers at Princeton and Google",
        "c": "Microsoft researchers",
        "d": "Meta researchers"
      },
      "answer": "b"
    },
    {
      "question": "What is the typical structure of ReAct prompts?",
      "options": {
        "a": "Input-output sequences",
        "b": "Thought-action-observation trio",
        "c": "Question-answer pairs",
        "d": "Code-comment blocks"
      },
      "answer": "b"
    },
    {
      "question": "What is crucial for guiding LLMs in the ReAct framework?",
      "options": {
        "a": "A defined set of allowed actions",
        "b": "Unlimited actions",
        "c": "Random action selection",
        "d": "Human intervention for each action"
      },
      "answer": "a"
    },
    {
      "question": "What does LangChain provide for working with LLMs?",
      "options": {
        "a": "Hardware optimization",
        "b": "Data storage solutions",
        "c": "Modular components including prompt templates and tools for external data interaction",
        "d": "Network security"
      },
      "answer": "c"
    },
    {
      "question": "What types of components does LangChain support?",
      "options": {
        "a": "Only prompt templates",
        "b": "Only custom agents",
        "c": "Both predefined chains for common tasks and agents for dynamic decision-making",
        "d": "Only predefined chains"
      },
      "answer": "c"
    },
    {
      "question": "Which models are preferred for advanced prompting techniques?",
      "options": {
        "a": "Smaller models for efficiency",
        "b": "Any model size works equally",
        "c": "Medium-sized models only",
        "d": "Larger models for better performance"
      },
      "answer": "d"
    },
    {
      "question": "What does the infrastructure layer provide for LLM applications?",
      "options": {
        "a": "Data analysis tools",
        "b": "User interfaces only",
        "c": "Security protocols only",
        "d": "Compute, storage, and network resources to host LLMs and application components"
      },
      "answer": "d"
    },
    {
      "question": "What is important for improving model performance over time?",
      "options": {
        "a": "Ignoring user feedback",
        "b": "Avoiding external data sources",
        "c": "Using only automated systems",
        "d": "Mechanisms for capturing outputs and gathering user feedback for fine-tuning"
      },
      "answer": "d"
    },
    {
      "question": "What are essential components of the final application layer?",
      "options": {
        "a": "Data storage only",
        "b": "Network hardware only",
        "c": "Only the model",
        "d": "User interface and security components"
      },
      "answer": "d"
    },
    {
      "question": "What perspective should be maintained about the model's role in the overall architecture?",
      "options": {
        "a": "The model is just one part of the overall architecture",
        "b": "The model is not important",
        "c": "The model is the entire application",
        "d": "Only the model matters for functionality"
      },
      "answer": "a"
    },
    {
      "question": "What is the primary goal of RLHF (Reinforcement Learning from Human Feedback)?",
      "options": {
        "a": "To eliminate the need for training data",
        "b": "To make models faster",
        "c": "To reduce model size",
        "d": "To align model outputs with human preferences and values"
      },
      "answer": "d"
    },
    {
      "question": "What happens during the iterative updates in the RL process?",
      "options": {
        "a": "Data is deleted",
        "b": "The reward value is used to update the LLM's weights through reinforcement learning algorithms",
        "c": "Human feedback is ignored",
        "d": "New models are created from scratch"
      },
      "answer": "b"
    },
    {
      "question": "What determines when the iterative RL process stops?",
      "options": {
        "a": "When the model meets specific evaluation criteria or reaches maximum steps",
        "b": "Random timing",
        "c": "When computational resources are exhausted",
        "d": "After exactly 100 iterations"
      },
      "answer": "a"
    },
    {
      "question": "What is a key advantage of using Constitutional AI over traditional RLHF?",
      "options": {
        "a": "It requires less computational power",
        "b": "It works only with small models",
        "c": "It's faster to implement",
        "d": "It helps mitigate unintended consequences and reduces reliance on human feedback"
      },
      "answer": "d"
    },
    {
      "question": "What is chain of thought prompting?",
      "options": {
        "a": "Random question asking",
        "b": "Breaking problems into smaller, human-like reasoning steps",
        "c": "Avoiding reasoning",
        "d": "Single-step solutions"
      },
      "answer": "b"
    }
  ]
}